{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is added by sphinx-gallery\n",
    "# It can be customized to whatever you like\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.14.0-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 740.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\d062732\\appdata\\local\\jupyterlabdesktopappserver\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\d062732\\appdata\\local\\jupyterlabdesktopappserver\\lib\\site-packages (from torchvision) (4.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\d062732\\appdata\\local\\jupyterlabdesktopappserver\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\d062732\\appdata\\local\\jupyterlabdesktopappserver\\lib\\site-packages (from torchvision) (1.23.4)\n",
      "Requirement already satisfied: torch==1.13.0 in c:\\users\\d062732\\appdata\\local\\jupyterlabdesktopappserver\\lib\\site-packages (from torchvision) (1.13.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\d062732\\appdata\\local\\jupyterlabdesktopappserver\\lib\\site-packages (from requests->torchvision) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\d062732\\appdata\\local\\jupyterlabdesktopappserver\\lib\\site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\d062732\\appdata\\local\\jupyterlabdesktopappserver\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\d062732\\appdata\\local\\jupyterlabdesktopappserver\\lib\\site-packages (from requests->torchvision) (2.1.1)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantum GANs {#quantum_gans}\n",
    "============\n",
    "\n",
    "::: {.meta}\n",
    ":property=\\\"og:description\\\": Explore quantum GANs to generate\n",
    "hand-written digits of zero :property=\\\"og:image\\\":\n",
    "<https://pennylane.ai/qml/_images/patch.jpeg>\n",
    ":::\n",
    "\n",
    "::: {.related}\n",
    "tutorial\\_QGAN Quantum generative adversarial networks with Cirq +\n",
    "TensorFlow\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Author: James Ellis --- Posted: 01 February 2022. Last updated: 27\n",
    "January 2022.*\n",
    "\n",
    "In this tutorial, we will explore quantum GANs to generate hand-written\n",
    "digits of zero. We will first cover the theory of the classical case,\n",
    "then extend to a quantum method recently proposed in the literature. If\n",
    "you have no experience with GANs, particularly in PyTorch, you might\n",
    "find [PyTorch\\'s\n",
    "tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\n",
    "useful since it serves as the foundation for what is to follow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks (GANs)\n",
    "======================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of generative adversarial networks (GANs) is to generate data\n",
    "that resembles the original data used in training. To achieve this, we\n",
    "train two neural networks simulatenously: a generator and a\n",
    "discriminator. The job of the generator is to create fake data which\n",
    "imitates the real training dataset. On the otherhand, the discriminator\n",
    "acts like a detective trying to discern real from fake data. During the\n",
    "training process, both players iteratively improve with one another. By\n",
    "the end, the generator should hopefully generate new data very similar\n",
    "to the training dataset.\n",
    "\n",
    "Specifically, the training dataset represents samples drawn from some\n",
    "unknown data distribution $P_{data}$, and the generator has the job of\n",
    "trying to capture this distribution. The generator, $G$, starts from\n",
    "some initial latent distribution, $P_z$, and maps it to $P_g = G(P_z)$.\n",
    "The best solution would be for $P_g = P_{data}$. However, this point is\n",
    "rarely achieved in practice apart from in the most simple tasks.\n",
    "\n",
    "Both the discriminator, $D$, and generator, $G$, play in a 2-player\n",
    "minimax game. The discriminator tries to maximise the probability of\n",
    "discerning real from fake data, while the generator tries to minimise\n",
    "the same probability. The value function for the game is summarised by,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\begin{align}\n",
    "\\min_G \\max_D V(D,G) &= \\mathbb{E}_{\\boldsymbol{x}\\sim p_{data}}[\\log D(\\boldsymbol{x})] \\\\\n",
    "    & ~~ + \\mathbb{E}_{\\boldsymbol{z}\\sim p_{\\boldsymbol{z}}}[\\log(1 - D(G(\\boldsymbol{z}))]\n",
    "\\end{align}\n",
    "\\end{aligned}$$\n",
    "\n",
    "-   $\\boldsymbol{x}$: real data sample\n",
    "-   $\\boldsymbol{z}$: latent vector\n",
    "-   $D(\\boldsymbol{x})$: probability of the discriminator classifying\n",
    "    real data as real\n",
    "-   $G(\\boldsymbol{z})$: fake data\n",
    "-   $D(G(\\boldsymbol{z}))$: probability of discriminator classifying\n",
    "    fake data as real\n",
    "\n",
    "In practice, the two networks are trained iteratively, each with a\n",
    "separate loss function to be minimised,\n",
    "\n",
    "$$L_D = -[y \\cdot \\log(D(x)) + (1-y)\\cdot \\log(1-D(G(z)))]$$\n",
    "\n",
    "$$L_G = [(1-y) \\cdot \\log(1-D(G(z)))]$$\n",
    "\n",
    "where $y$ is a binary label for real ($y=1$) or fake ($y=0$) data. In\n",
    "practice, generator training is shown to be more stable when made to\n",
    "maximise $\\log(D(G(z)))$ instead of minimising $\\log(1-D(G(z)))$. Hence,\n",
    "the generator loss function to be minimised becomes,\n",
    "\n",
    "$$L_G = -[(1-y) \\cdot \\log(D(G(z)))]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantum GANs: The Patch Method\n",
    "==============================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we re-create one of the quantum GAN methods presented\n",
    "by Huang et al.: the patch method. This method uses several quantum\n",
    "generators, with each sub-generator, $G^{(i)}$, responsible for\n",
    "constructing a small patch of the final image. The final image is\n",
    "contructed by concatenting all of the patches together as shown below.\n",
    "\n",
    "![](../demonstrations/quantum_gans/patch.jpeg){.align-center\n",
    "width=\"90.0%\"}\n",
    "\n",
    "The main advantage of this method is that it is particulary suited to\n",
    "situations where the number of available qubits are limited. The same\n",
    "quantum device can be used for each sub-generator in an iterative\n",
    "fashion, or execution of the generators can be parallelised across\n",
    "multiple devices.\n",
    "\n",
    "::: {.note}\n",
    "::: {.title}\n",
    "Note\n",
    ":::\n",
    "\n",
    "In this tutorial, parenthesised superscripts are used to denote\n",
    "individual objects as part of a collection.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module Imports\n",
    "==============\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pennylane as qml\n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "====\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the introduction, we will use a [small\n",
    "dataset](https://archive.ics.uci.edu/ml/datasets/optical+recognition+of+handwritten+digits)\n",
    "of handwritten zeros. First, we need to create a custom dataloader for\n",
    "this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class DigitsDataset(Dataset):\n",
    "    \"\"\"Pytorch dataloader for the Optical Recognition of Handwritten Digits Data Set\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, label=0, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.csv_file = csv_file\n",
    "        self.transform = transform\n",
    "        self.df = self.filter_by_label(label)\n",
    "\n",
    "    def filter_by_label(self, label):\n",
    "        # Use pandas to return a dataframe of only zeros\n",
    "        df = pd.read_csv(self.csv_file)\n",
    "        df = df.loc[df.iloc[:, -1] == label]\n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = self.df.iloc[idx, :-1] / 16\n",
    "        image = np.array(image)\n",
    "        image = image.astype(np.float32).reshape(8, 8)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Return image and label\n",
    "        return image, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define some variables and create the dataloader instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_size = 8  # Height / width of the square images\n",
    "batch_size = 1\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = DigitsDataset(csv_file=\"C:/Users/D062732/Downloads/optdigits.tra\", transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\\'s visualize some of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,2))\n",
    "\n",
    "for i in range(8):\n",
    "    image = dataset[i][0].reshape(image_size,image_size)\n",
    "    plt.subplot(1,8,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image.numpy(), cmap='gray')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the Discriminator\n",
    "==============================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the discriminator, we use a fully connected neural network with two\n",
    "hidden layers. A single output is sufficient to represent the\n",
    "probability of an input being classified as real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Fully connected classical discriminator\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # Inputs to first hidden layer (num_input_features -> 64)\n",
    "            nn.Linear(image_size * image_size, 64),\n",
    "            nn.ReLU(),\n",
    "            # First hidden layer (64 -> 16)\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            # Second hidden layer (16 -> output)\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the Generator\n",
    "==========================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sub-generator, $G^{(i)}$, shares the same circuit architecture as\n",
    "shown below. The overall quantum generator consists of $N_G$\n",
    "sub-generators, each consisting of $N$ qubits. The process from latent\n",
    "vector input to image output can be split into four distinct sections:\n",
    "state embedding, parameterisation, non-linear transformation, and\n",
    "post-processing. Each of the following sections below refer to a single\n",
    "iteration of the training process to simplify the discussion.\n",
    "\n",
    "![](../demonstrations/quantum_gans/qcircuit.jpeg){.align-center\n",
    "width=\"90.0%\"}\n",
    "\n",
    "**1) State Embedding**\n",
    "\n",
    "A latent vector, $\\boldsymbol{z}\\in\\mathbb{R}^N$, is sampled from a\n",
    "uniform distribution in the interval $[0,\\pi/2)$. All sub-generators\n",
    "receive the same latent vector which is then embedded using RY gates.\n",
    "\n",
    "**2) Parameterised Layers**\n",
    "\n",
    "The parameterised layer consists of parameterised RY gates followed by\n",
    "control Z gates. This layer is repeated $D$ times in total.\n",
    "\n",
    "**3) Non-Linear Transform**\n",
    "\n",
    "Quantum gates in the circuit model are unitary which, by definition,\n",
    "linearly transform the quantum state. A linear mapping between the\n",
    "latent and generator distribution would be suffice for only the most\n",
    "simple generative tasks, hence we need non-linear transformations. We\n",
    "will use ancillary qubits to help.\n",
    "\n",
    "For a given sub-generator, the pre-measurement quantum state is given\n",
    "by,\n",
    "\n",
    "$$|\\Psi(z)\\rangle = U_{G}(\\theta)|\\boldsymbol{z}\\rangle$$\n",
    "\n",
    "where $U_{G}(\\theta)$ represents the overall unitary of the\n",
    "parameterised layers. Let us inspect the state when we take a partial\n",
    "measurment, $\\Pi$, and trace out the ancillary subsystem, $\\mathcal{A}$,\n",
    "\n",
    "$$\\rho(\\boldsymbol{z}) = \\frac{\\text{Tr}_{\\mathcal{A}}(\\Pi \\otimes \\mathbb{I} |\\Psi(z)\\rangle \\langle \\Psi(\\boldsymbol{z})|) }{\\text{Tr}(\\Pi \\otimes \\mathbb{I} |\\Psi(\\boldsymbol{z})\\rangle \\langle \\Psi(\\boldsymbol{z})|))} = \\frac{\\text{Tr}_{\\mathcal{A}}(\\Pi \\otimes \\mathbb{I} |\\Psi(\\boldsymbol{z})\\rangle \\langle \\Psi(\\boldsymbol{z})|) }{\\langle \\Psi(\\boldsymbol{z})| \\Pi \\otimes \\mathbb{I} |\\Psi(\\boldsymbol{z})\\rangle}$$\n",
    "\n",
    "The post-measurement state, $\\rho(\\boldsymbol{z})$, is dependent on\n",
    "$\\boldsymbol{z}$ in both the numerator and denominator. This means the\n",
    "state has been non-linearly transformed! For this tutorial,\n",
    "$\\Pi = (|0\\rangle \\langle0|)^{\\otimes N_A}$, where $N_A$ is the number\n",
    "of ancillary qubits in the system.\n",
    "\n",
    "With the remaining data qubits, we measure the probability of\n",
    "$\\rho(\\boldsymbol{z})$ in each computational basis state, $P(j)$, to\n",
    "obtain the sub-generator output, $\\boldsymbol{g}^{(i)}$,\n",
    "\n",
    "$$\\boldsymbol{g}^{(i)} = [P(0), P(1), ... ,P(2^{N-N_A} - 1)]$$\n",
    "\n",
    "**4) Post Processing**\n",
    "\n",
    "Due to the normalisation constraint of the measurment, all elements in\n",
    "$\\boldsymbol{g}^{(i)}$ must sum to one. This is a problem if we are to\n",
    "use $\\boldsymbol{g}^{(i)}$ as the pixel intensity values for our patch.\n",
    "For example, imagine a hypothetical situation where a patch of full\n",
    "intensity pixels was the target. The best patch a sub-generator could\n",
    "produce would be a patch of pixels all at a magnitude of\n",
    "$\\frac{1}{2^{N-N_A}}$. To alleviate this constraint, we apply a\n",
    "post-processing technique to each patch,\n",
    "\n",
    "$$\\boldsymbol{\\tilde{x}^{(i)}} = \\frac{\\boldsymbol{g}^{(i)}}{\\max_{k}\\boldsymbol{g}_k^{(i)}}$$\n",
    "\n",
    "Therefore, the final image, $\\boldsymbol{\\tilde{x}}$, is given by\n",
    "\n",
    "$$\\boldsymbol{\\tilde{x}} = [\\boldsymbol{\\tilde{x}^{(1)}}, ... ,\\boldsymbol{\\tilde{x}^{(N_G)}}]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Quantum variables\n",
    "n_qubits = 5  # Total number of qubits / N\n",
    "n_a_qubits = 1  # Number of ancillary qubits / N_A\n",
    "q_depth = 6  # Depth of the parameterised quantum circuit / D\n",
    "n_generators = 4  # Number of subgenerators for the patch method / N_G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the quantum device we want to use, along with any\n",
    "available CUDA GPUs (if available).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Quantum simulator\n",
    "dev = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
    "# Enable CUDA device if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device is\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the quantum circuit and measurement process described\n",
    "above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@qml.qnode(dev, interface=\"torch\", diff_method=\"parameter-shift\")\n",
    "def quantum_circuit(noise, weights):\n",
    "\n",
    "    weights = weights.reshape(q_depth, n_qubits)\n",
    "\n",
    "    # Initialise latent vectors\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(noise[i], wires=i)\n",
    "\n",
    "    # Repeated layer\n",
    "    for i in range(q_depth):\n",
    "        # Parameterised layer\n",
    "        for y in range(n_qubits):\n",
    "            qml.RY(weights[i][y], wires=y)\n",
    "\n",
    "        # Control Z gates\n",
    "        for y in range(n_qubits - 1):\n",
    "            qml.CZ(wires=[y, y + 1])\n",
    "\n",
    "    return qml.probs(wires=list(range(n_qubits)))\n",
    "\n",
    "\n",
    "# For further info on how the non-linear transform is implemented in Pennylane\n",
    "# https://discuss.pennylane.ai/t/ancillary-subsystem-measurement-then-trace-out/1532\n",
    "def partial_measure(noise, weights):\n",
    "    # Non-linear Transform\n",
    "    probs = quantum_circuit(noise, weights)\n",
    "    probsgiven0 = probs[: (2 ** (n_qubits - n_a_qubits))]\n",
    "    probsgiven0 /= torch.sum(probs)\n",
    "\n",
    "    # Post-Processing\n",
    "    probsgiven = probsgiven0 / torch.max(probsgiven0)\n",
    "    return probsgiven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a quantum generator class to use during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class PatchQuantumGenerator(nn.Module):\n",
    "    \"\"\"Quantum generator class for the patch method\"\"\"\n",
    "\n",
    "    def __init__(self, n_generators, q_delta=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_generators (int): Number of sub-generators to be used in the patch method.\n",
    "            q_delta (float, optional): Spread of the random distribution for parameter initialisation.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.q_params = nn.ParameterList(\n",
    "            [\n",
    "                nn.Parameter(q_delta * torch.rand(q_depth * n_qubits), requires_grad=True)\n",
    "                for _ in range(n_generators)\n",
    "            ]\n",
    "        )\n",
    "        self.n_generators = n_generators\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Size of each sub-generator output\n",
    "        patch_size = 2 ** (n_qubits - n_a_qubits)\n",
    "\n",
    "        # Create a Tensor to 'catch' a batch of images from the for loop. x.size(0) is the batch size.\n",
    "        images = torch.Tensor(x.size(0), 0).to(device)\n",
    "\n",
    "        # Iterate over all sub-generators\n",
    "        for params in self.q_params:\n",
    "\n",
    "            # Create a Tensor to 'catch' a batch of the patches from a single sub-generator\n",
    "            patches = torch.Tensor(0, patch_size).to(device)\n",
    "            for elem in x:\n",
    "                q_out = partial_measure(elem, params).float().unsqueeze(0)\n",
    "                patches = torch.cat((patches, q_out))\n",
    "\n",
    "            # Each batch of patches is concatenated with each other to create a batch of images\n",
    "            images = torch.cat((images, patches), 1)\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "========\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\\'s define learning rates and number of iterations for the training\n",
    "process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lrG = 0.3  # Learning rate for the generator\n",
    "lrD = 0.01  # Learning rate for the discriminator\n",
    "num_iter = 500  # Number of training iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now putting everything together and executing the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator().to(device)\n",
    "generator = PatchQuantumGenerator(n_generators).to(device)\n",
    "\n",
    "# Binary cross entropy\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimisers\n",
    "optD = optim.SGD(discriminator.parameters(), lr=lrD)\n",
    "optG = optim.SGD(generator.parameters(), lr=lrG)\n",
    "\n",
    "real_labels = torch.full((batch_size,), 1.0, dtype=torch.float, device=device)\n",
    "fake_labels = torch.full((batch_size,), 0.0, dtype=torch.float, device=device)\n",
    "\n",
    "# Fixed noise allows us to visually track the generated images throughout training\n",
    "fixed_noise = torch.rand(8, n_qubits, device=device) * math.pi / 2\n",
    "\n",
    "# Iteration counter\n",
    "counter = 0\n",
    "\n",
    "# Collect images for plotting later\n",
    "results = []\n",
    "\n",
    "while True:\n",
    "    for i, (data, _) in enumerate(dataloader):\n",
    "\n",
    "        # Data for training the discriminator\n",
    "        data = data.reshape(-1, image_size * image_size)\n",
    "        real_data = data.to(device)\n",
    "\n",
    "        # Noise follwing a uniform distribution in range [0,pi/2)\n",
    "        noise = torch.rand(batch_size, n_qubits, device=device) * math.pi / 2\n",
    "        fake_data = generator(noise)\n",
    "\n",
    "        # Training the discriminator\n",
    "        discriminator.zero_grad()\n",
    "        outD_real = discriminator(real_data).view(-1)\n",
    "        outD_fake = discriminator(fake_data.detach()).view(-1)\n",
    "\n",
    "        errD_real = criterion(outD_real, real_labels)\n",
    "        errD_fake = criterion(outD_fake, fake_labels)\n",
    "        # Propagate gradients\n",
    "        errD_real.backward()\n",
    "        errD_fake.backward()\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "        optD.step()\n",
    "\n",
    "        # Training the generator\n",
    "        generator.zero_grad()\n",
    "        outD_fake = discriminator(fake_data).view(-1)\n",
    "        errG = criterion(outD_fake, real_labels)\n",
    "        errG.backward()\n",
    "        optG.step()\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "        # Show loss values         \n",
    "        if counter % 10 == 0:\n",
    "            print(f'Iteration: {counter}, Discriminator Loss: {errD:0.3f}, Generator Loss: {errG:0.3f}')\n",
    "            test_images = generator(fixed_noise).view(8,1,image_size,image_size).cpu().detach()\n",
    "            \n",
    "            # Save images every 50 iterations\n",
    "            if counter % 50 == 0:\n",
    "                results.append(test_images)\n",
    "\n",
    "        if counter == num_iter:\n",
    "            break\n",
    "    if counter == num_iter:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.note}\n",
    "::: {.title}\n",
    "Note\n",
    ":::\n",
    "\n",
    "You may have noticed `errG = criterion(outD_fake, real_labels)` and\n",
    "wondered why we don't use `fake_labels` instead of `real_labels`.\n",
    "However, this is simply a trick to be able to use the same `criterion`\n",
    "function for both the generator and discriminator. Using `real_labels`\n",
    "forces the generator loss function to use the $\\log(D(G(z))$ term\n",
    "instead of the $\\log(1 - D(G(z))$ term of the binary cross entropy loss\n",
    "function.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot how the generated images evolved throughout training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "outer = gridspec.GridSpec(5, 2, wspace=0.1)\n",
    "\n",
    "for i, images in enumerate(results):\n",
    "    inner = gridspec.GridSpecFromSubplotSpec(1, images.size(0),\n",
    "                    subplot_spec=outer[i])\n",
    "    \n",
    "    images = torch.squeeze(images, dim=1)\n",
    "    for j, im in enumerate(images):\n",
    "\n",
    "        ax = plt.Subplot(fig, inner[j])\n",
    "        ax.imshow(im.numpy(), cmap=\"gray\")\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if j==0:\n",
    "            ax.set_title(f'Iteration {50+i*50}', loc='left')\n",
    "        fig.add_subplot(ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgements\n",
    "================\n",
    "\n",
    "Many thanks to Karolis Špukas who I co-developed much of the code with.\n",
    "I also extend my thanks to Dr. Yuxuan Du for answering my questions\n",
    "regarding his paper. I am also indebited to the Pennylane community for\n",
    "their help over the past few years.\n",
    "\n",
    "References\n",
    "==========\n",
    "\n",
    "About the author\n",
    "================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
